{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Glider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dustin D. Gerrard\n",
    "### July 15, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glider import Glider\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# number of discrete state buckets\n",
    "NUM_BUCKETS = (10, 10, 5, 5, 5) # (x, y, x_dot, y_dot, phi)\n",
    "\n",
    "# number of discrete actions\n",
    "NUM_ACTIONS = 3 # phi_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_buckets = 1\n",
    "for i in range(len(NUM_BUCKETS)):\n",
    "    num_buckets = num_buckets*NUM_BUCKETS[i]\n",
    "num_buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_state_array = np.linspace(0, 150, num=NUM_BUCKETS[0], endpoint=False)\n",
    "y_state_array = np.linspace(0, 100, num=NUM_BUCKETS[1], endpoint=False)\n",
    "x_dot_state_array = np.linspace(0, 60, num=NUM_BUCKETS[2], endpoint=False)\n",
    "y_dot_state_array = np.linspace(0, 60, num=NUM_BUCKETS[3], endpoint=False)\n",
    "phi_state_array = np.linspace(-1.5, 1.5, num=NUM_BUCKETS[4], endpoint=False)\n",
    "all_state_arrays = (x_state_array, y_state_array, x_dot_state_array, y_dot_state_array, phi_state_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_state_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy_matrix = np.random.randint(low = 0, high = NUM_ACTIONS, size = NUM_BUCKETS)\n",
    "# state_action_matrix = np.zeros((NUM_ACTIONS, num_buckets))\n",
    "# policy_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_discrete_states(state_cont, all_state_arrays):\n",
    "    state_disc = [np.digitize(state_cont[0], all_state_arrays[0]), \n",
    "                     np.digitize(state_cont[1], all_state_arrays[1]), \n",
    "                     np.digitize(state_cont[2], all_state_arrays[2]),\n",
    "                     np.digitize(state_cont[3], all_state_arrays[3]),\n",
    "                     np.digitize(state_cont[4], all_state_arrays[4])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_epsilon(epsilon_init, epsilon_min, episode_step, num_steps_epsilon_decay):\n",
    "    epsilon = epsilon_init*np.power(0.9,(episode_step/num_steps_epsilon_decay))\n",
    "    if epsilon < epsilon_min:\n",
    "        epsilon = epsilon_min\n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_policy_matrix(policy_matrix):\n",
    "    \n",
    "    return policy_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main learning loop\n",
    "policy_matrix = np.random.randint(low = 0, high = NUM_ACTIONS, size = NUM_BUCKETS)\n",
    "state_action_matrix = np.zeros((NUM_ACTIONS, num_buckets))\n",
    "\n",
    "num_episodes = 100 # 10\n",
    "max_step = 10000 # 10k\n",
    "g = Glider()\n",
    "for episode in range(num_episodes):\n",
    "    if episode%1 == 0:\n",
    "        print('Episode: ' + str(episode))\n",
    "    epsilon = get_epsilon(0.99, 0.1, episode, 10)\n",
    "    Y_cont, X_cont, V_cont = [], [], []\n",
    "    episode_list = list()\n",
    "    total_reward = 0.0\n",
    "    state_cont = g.reset()\n",
    "    state_disc = get_discrete_states(state_cont, all_state_arrays)\n",
    "    for step in range(max_step):\n",
    "        action = 0.0\n",
    "        reward, done = g.step(action)\n",
    "        new_state_cont = g.get_state()\n",
    "        Y_cont.append(new_state_cont[2])\n",
    "        X_cont.append(new_state_cont[0])\n",
    "        V_cont.append(math.sqrt(new_state_cont[1]**2 + new_state_cont[3]**2))\n",
    "        new_state_disc = get_discrete_states(state_cont, all_state_arrays)\n",
    "        total_reward += reward\n",
    "        episode_list.append((state_cont, state_disc, action, reward))                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax([0,1,24,5,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.full((4), 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-41389fad42b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
